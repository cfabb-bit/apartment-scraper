name: Scrape All Sites Auto
on:
  workflow_dispatch:
  repository_dispatch:
    types: [scrape_all]
    
jobs:
  find-scrapers:
    runs-on: ubuntu-latest
    outputs:
      scrapers: ${{ steps.find-scrapers.outputs.scrapers }}
    steps:
      - uses: actions/checkout@v4
      - name: Find all scrapers
        id: find-scrapers
        run: |
          # Trova tutti i file scraper-*.js
        SCRAPERS=$(find . -maxdepth 1 -name "scraper-*.js" -type f | sed 's|./scraper-||' | sed 's|.js||' | sort | jq -R . | jq -s . | tr -d '\n')
          echo "scrapers=$SCRAPERS" >> $GITHUB_OUTPUT
          echo "üîç Scrapers trovati: $SCRAPERS"

  scrape-all:
    needs: find-scrapers  
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '18'
      - run: npm install
      - run: npx playwright install chromium
      
      - name: Scrape all sites sequentially
        run: |
          echo "üöÄ Iniziando scraping di tutti i siti..."
          
          # Converti JSON array in bash array
          SCRAPERS='${{ needs.find-scrapers.outputs.scrapers }}'
          echo "Raw scrapers: $SCRAPERS"
          
          # Estrai i nomi degli scraper
          SCRAPER_LIST=$(echo "$SCRAPERS" | jq -r '.[]')
          
          echo "üìã Siti da scrapare:"
          echo "$SCRAPER_LIST"
          
          # Esegui ogni scraper
          for SCRAPER in $SCRAPER_LIST; do
            echo ""
            echo "üîÑ === SCRAPANDO: $SCRAPER ==="
            
            # Esegui scraper con timeout e debug
            echo "üîÑ Avvio scraper-${SCRAPER}.js..."
            START_TIME=$(date +%s)
            
            if timeout 300 node "scraper-${SCRAPER}.js"; then
              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))
              echo "‚úÖ $SCRAPER completato con successo in ${DURATION}s"
              
              # Determina nome file risultato
              RESULT_FILE="results.json"
              if [ "$SCRAPER" = "immobilien" ]; then
                RESULT_FILE="results-immobilien.json"
              fi
              
              # Verifica se file risultato esiste
              if [ -f "$RESULT_FILE" ]; then
                echo "üìä File risultato trovato: $RESULT_FILE"
                
                # Mostra summary se possibile
                if command -v jq &> /dev/null && jq empty "$RESULT_FILE" 2>/dev/null; then
                  COUNT=$(jq -r '.count // .data | length // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
                  SUCCESS=$(jq -r '.success // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
                  TIMESTAMP=$(jq -r '.timestamp // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
                  ERROR_MSG=$(jq -r '.error // "Nessuno"' "$RESULT_FILE" 2>/dev/null || echo "Nessuno")
                  
                  echo "   üìä RISULTATI $SCRAPER:"
                  echo "      Appartamenti: $COUNT"
                  echo "      Successo: $SUCCESS"  
                  echo "      Timestamp: $TIMESTAMP"
                  echo "      Errori: $ERROR_MSG"
                  
                  # Se zero risultati, suggerisci debug
                  if [ "$COUNT" = "0" ] || [ "$COUNT" = "N/A" ]; then
                    echo "   ‚ö†Ô∏è  ZERO RISULTATI - possibili cause:"
                    echo "      - Filtri troppo restrittivi (prezzo, zona)"
                    echo "      - Selettori CSS cambiati"
                    echo "      - Sito ha cambiato struttura"
                    echo "      - Rate limiting o bot detection"
                  fi
                else
                  echo "   ‚ùå File presente ma non JSON valido o corrotto"
                  echo "   üîç Prime righe del file:"
                  head -n 5 "$RESULT_FILE" || echo "   Impossibile leggere file"
                fi
                
                # Upload a Gist
                echo "üì§ Uploading $SCRAPER a Gist..."
                
                # Determina Gist ID basato sul nome sito
                case "$SCRAPER" in
                  "gewobag")
                    GIST_ID="${{ secrets.GIST_ID_GEWOBAG }}"
                    ;;
                  "stadtundland")
                    GIST_ID="${{ secrets.GIST_ID }}"
                    ;;
                  "immobilien")
                    GIST_ID="${{ secrets.GIST_ID_IMMOBILIEN }}"
                    ;;
                  *)
                    # Per futuri scraper, cerca secret dinamico
                    SECRET_NAME="GIST_ID_$(echo $SCRAPER | tr '[:lower:]' '[:upper:]')"
                    echo "‚ö†Ô∏è  Secret $SECRET_NAME non configurato per $SCRAPER"
                    echo "üí° Aggiungi $SECRET_NAME ai repository secrets!"
                    GIST_ID=""
                    ;;
                esac
                
                if [ -n "$GIST_ID" ]; then
                  # Upload con gestione errori
                  HTTP_STATUS=$(curl -w "%{http_code}" -s -o /tmp/gist_response -X PATCH \
                    -H "Authorization: token ${{ secrets.GIST_TOKEN }}" \
                    -H "Content-Type: application/json" \
                    -d "{\"files\":{\"${SCRAPER}.json\":{\"content\":\"$(cat $RESULT_FILE | jq -c . | sed 's/\"/\\\"/g')\"}}}" \
                    "https://api.github.com/gists/$GIST_ID")
                  
                  if [ "$HTTP_STATUS" = "200" ]; then
                    echo "‚úÖ Gist aggiornato con successo per $SCRAPER"
                  else
                    echo "‚ùå Errore upload Gist per $SCRAPER (HTTP $HTTP_STATUS)"
                    cat /tmp/gist_response
                  fi
                else
                  echo "‚è≠Ô∏è  Skip upload Gist per $SCRAPER (ID mancante)"
                fi
                
              else
                echo "‚ùå File risultato non trovato: $RESULT_FILE"
              fi
              
              else
                echo "‚ùå $SCRAPER fallito in ${DURATION}s"
                echo "üîç DEBUG INFO per $SCRAPER:"
                echo "   - Check logs sopra per errori specifici"
                echo "   - Possibili cause: timeout, selettori CSS cambiati, sito bloccato"
                echo "   - File risultato atteso: $RESULT_FILE"
                
                # Mostra eventuali file di debug creati
                if ls debug-* 2>/dev/null; then
                  echo "   - File debug trovati:"
                  ls -la debug-*
                fi
              fi
            
            echo "‚è±Ô∏è  Pausa 10 secondi prima del prossimo scraper..."
            sleep 10
            
          done
          
          echo ""
          echo "üéâ === SCRAPING COMPLETATO ==="
          echo "Scrapers processati: $SCRAPER_LIST"
          
  summary:
    needs: [find-scrapers, scrape-all]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Final Summary
        run: |
          echo "üéØ === RIEPILOGO FINALE ==="
          echo "Scrapers trovati: ${{ needs.find-scrapers.outputs.scrapers }}"
          echo ""
          echo "üìä Per vedere i risultati:"
          echo "- Vai su https://gist.github.com"  
          echo "- Controlla i Gist aggiornati"
          echo "- n8n pu√≤ fare GET sui soliti Gist ID"
          echo ""
          echo "üîÑ Per aggiungere nuovi siti:"
          echo "1. Crea scraper-NOMESITO.js" 
          echo "2. Aggiungi GIST_ID_NOMESITO ai secrets"
          echo "3. Questo workflow li trova automaticamente!"
